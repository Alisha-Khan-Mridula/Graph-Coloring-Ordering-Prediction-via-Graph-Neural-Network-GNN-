{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLlvpmX5sivy7GenUyPabM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Modified Attention Mechanism based on the requirements**"
      ],
      "metadata": {
        "id": "lkfq3unnfN2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0waLf9kzGs_U",
        "outputId": "59596219-2d1a-482c-e42b-a44e3e47a402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and extracted dataset to cora\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import networkx as nx\n",
        "import tarfile\n",
        "import urllib.request\n",
        "def download_dataset(url, dest_folder=\"cora\"):\n",
        "    if not os.path.exists(dest_folder):\n",
        "        os.makedirs(dest_folder)\n",
        "\n",
        "    file_path = os.path.join(dest_folder, \"cora.tgz\")\n",
        "\n",
        "    # Download the dataset\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "    # Extract the tar file\n",
        "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=dest_folder)\n",
        "\n",
        "    print(f\"Downloaded and extracted dataset to {dest_folder}\")\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\"\n",
        "download_dataset(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "path = \"cora/cora\"\n",
        "nodes_data = np.genfromtxt(os.path.join(path, \"cora.content\"), dtype=str)\n",
        "edges = np.genfromtxt(os.path.join(path, \"cora.cites\"), dtype=int)\n",
        "\n",
        "G = nx.Graph()\n",
        "node_features = []\n",
        "node_id_map = {}\n",
        "index = 0\n",
        "\n",
        "#Extrating node features\n",
        "for node in nodes_data:\n",
        "  node_id = int(node[0])\n",
        "  feature = np.array(node[1:-1], dtype=np.float32)\n",
        "\n",
        "  # Add node to graph with features (without label)\n",
        "  G.add_node(node_id, feature=feature)\n",
        "  node_features.append(feature)\n",
        "  # Mapping node IDs\n",
        "  node_id_map[node_id] = index\n",
        "  index += 1\n",
        "\n",
        "\n",
        "features = np.array(node_features)\n",
        "\n",
        "row_sum = np.array(features.sum(1))\n",
        "row_inv = np.power(row_sum, -1).flatten()\n",
        "row_inv[np.isinf(row_inv)] = 0.\n",
        "row_mat_inv = sp.diags(row_inv)\n",
        "features = row_mat_inv.dot(features)\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfgTyaUlfyju",
        "outputId": "eac882db-e99a-4a7d-b712-93494fdc6b46"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_nodes = features.shape[0]\n",
        "ft_size = features.shape[1]\n",
        "print(nb_nodes)\n",
        "print(ft_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwE55UYHk24Q",
        "outputId": "70f4211a-3705-4a23-e102-aee066a20bd9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2708\n",
            "1433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "hid_units = [8] #No of output size\n",
        "n_heads = [8, 1]\n",
        "nonlinearity = tf.nn.elu\n",
        "residual = False"
      ],
      "metadata": {
        "id": "BVo2iYtQlErk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_input = tf.keras.Input(shape=(nb_nodes, ft_size), batch_size=batch_size)\n",
        "bias_input = tf.keras.Input(shape=(nb_nodes, nb_nodes), batch_size=batch_size)\n",
        "ffd_drop = 0.0\n",
        "attn_drop = 0.0"
      ],
      "metadata": {
        "id": "JmDiBtYSlOnw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Add\n",
        "seq = feature_input\n",
        "\n",
        "\n",
        "conv1d = layers.Conv1D #layers.Conv1D\n",
        "\n",
        "# Multi head Attention Mechanism (Average mechanism has been used)\n",
        "def attn_head(seq, bias_mat, out_sz, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
        "  with tf.name_scope('my_attn'):\n",
        "          # Apply dropout if needed\n",
        "          if in_drop != 0.0:\n",
        "              seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
        "\n",
        "          seq_fts = layers.Conv1D(filters=out_sz, kernel_size=1, use_bias=False)(seq)\n",
        "\n",
        "          # Self-attention mechanism\n",
        "          f_1 = layers.Conv1D(filters=1, kernel_size=1)(seq_fts)\n",
        "          f_2 = layers.Conv1D(filters=1, kernel_size=1)(seq_fts)\n",
        "\n",
        "          f_2_transposed = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(f_2)\n",
        "          logits = layers.Add()([f_1, f_2_transposed])\n",
        "\n",
        "          leaky_relu = layers.LeakyReLU(alpha=0.2)\n",
        "          logits_with_bias = layers.Add()([logits, bias_mat])\n",
        "          coefs = layers.Activation('softmax')(leaky_relu(logits_with_bias))\n",
        "\n",
        "          if coef_drop != 0.0:\n",
        "              coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
        "\n",
        "\n",
        "          return coefs\n",
        "\n",
        "class CustomActivation(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.leaky_relu(inputs, alpha=0.2)\n",
        "\n",
        "length = int(n_heads[0])\n",
        "attention_weights = []\n",
        "for i in range(length):\n",
        "  attention_weights.append(attn_head(seq, bias_mat=bias_input,\n",
        "                out_sz=hid_units[0], activation=nonlinearity,\n",
        "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
        "\n",
        "attention_weights_avg = Add()(attention_weights)/n_heads[0]\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "oreAF9H5ldoB"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model\n",
        "model = tf.keras.Model(inputs=[feature_input, bias_input], outputs=[attention_weights_avg])\n",
        "feature_input_pred = tf.random.normal(shape=(batch_size, nb_nodes, ft_size))\n",
        "bias_input_pred = tf.random.normal(shape=(batch_size, nb_nodes, nb_nodes))\n",
        "attention_weights  = model.predict([feature_input_pred, bias_input_pred])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L23vQnqWmoNp",
        "outputId": "70b45dd8-274c-403d-be69-2b4972bf93a6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f06db3e4700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly Connected Neighbors\n",
        "neighbors = edges[(edges[:, 0] == 6213 ) | (edges[:, 1] == 6213 )]\n",
        "\n",
        "#direct_neighbors = np.unique(neighbors[neighbors != 6213 ])\n",
        "print(neighbors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_CQasi2mz4h",
        "outputId": "8cee4dfa-f5f2-4d66-a94e-714ce5885e75"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    887    6213]\n",
            " [   4584    6213]\n",
            " [   6151    6213]\n",
            " [   6213   10796]\n",
            " [   6213 1103315]\n",
            " [   6213 1105764]\n",
            " [   6213 1106406]\n",
            " [   6213 1106547]\n",
            " [   6213 1106966]\n",
            " [   6213 1107355]\n",
            " [   6213 1107455]\n",
            " [   6213 1108329]\n",
            " [   6213 1109957]\n",
            " [   6213 1111052]\n",
            " [   6213 1111230]\n",
            " [   6213 1111304]\n",
            " [   6213 1113182]\n",
            " [   6213 1113459]\n",
            " [   6213 1113614]\n",
            " [   6213 1116347]\n",
            " [   6213 1117760]\n",
            " [   6213 1117942]\n",
            " [   6213 1120020]\n",
            " [   6213 1122425]\n",
            " [   6213 1123553]\n",
            " [   6213 1128267]\n",
            " [   6213 1129096]\n",
            " [   6213 1129243]\n",
            " [   6213 1130567]\n",
            " [   6213  124064]\n",
            " [   6213   12576]\n",
            " [   6213     128]\n",
            " [   6213   13193]\n",
            " [   6213  134128]\n",
            " [   6213    1385]\n",
            " [   6213   13960]\n",
            " [   6213  153598]\n",
            " [   6213  161221]\n",
            " [   6213   17208]\n",
            " [   6213  193742]\n",
            " [   6213  195361]\n",
            " [   6213   20526]\n",
            " [   6213   20534]\n",
            " [   6213  218410]\n",
            " [   6213   23774]\n",
            " [   6213  241133]\n",
            " [   6213   28202]\n",
            " [   6213   28227]\n",
            " [   6213   28278]\n",
            " [   6213   28350]\n",
            " [   6213   28471]\n",
            " [   6213   28485]\n",
            " [   6213  293271]\n",
            " [   6213  299195]\n",
            " [   6213  299197]\n",
            " [   6213  353541]\n",
            " [   6213   35863]\n",
            " [   6213   38480]\n",
            " [   6213  399339]\n",
            " [   6213     434]\n",
            " [   6213   55968]\n",
            " [   6213   58540]\n",
            " [   6213  595056]\n",
            " [   6213    6151]\n",
            " [   6213    6155]\n",
            " [   6213    6163]\n",
            " [   6213    6184]\n",
            " [   6213    6210]\n",
            " [   6213    6214]\n",
            " [   6213    6215]\n",
            " [   6213    6220]\n",
            " [   6213    6224]\n",
            " [   6213  628667]\n",
            " [   6213  628668]\n",
            " [   6213  628815]\n",
            " [   6213  672064]\n",
            " [   6213    7419]\n",
            " [   6213  755217]\n",
            " [   6213   91975]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Node_ID to index\n",
        "node_index_1 = 5348 #neighbors[0][0]\n",
        "node_index_2 = 2695     #neighbors[0][1]\n",
        "# From Node_ID to Index\n",
        "node_id_1 = node_index_1\n",
        "node_id_2 = node_index_2\n",
        "if node_id_1 in node_id_map:\n",
        "    index_1 = node_id_map[node_id_1]\n",
        "    print(index_1)\n",
        "else:\n",
        "    print(\"Error\")\n",
        "if node_id_2 in node_id_map:\n",
        "    index_2 = node_id_map[node_id_2]\n",
        "    print(index_2)\n",
        "else:\n",
        "  print(\"Error\")\n",
        "\n",
        "#Attention Weights\n",
        "print(f\"Attention weights for node(Index value) {index_1} to {index_2}\")\n",
        "print(attention_weights[0, index_1, index_2 ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-iVDLBAnn4Q",
        "outputId": "589c6294-1666-41bb-dba0-02a4d9e0d67f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "359\n",
            "2498\n",
            "Attention weights for node(Index value) 359 to 2498\n",
            "7.578681e-05\n"
          ]
        }
      ]
    }
  ]
}